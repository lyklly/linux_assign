import os
import json
import time
import tracemalloc
from collections import defaultdict, Counter
from tqdm import tqdm
from tree_sitter import Language, Parser
import tree_sitter_c as tsc
import sys
import gc
sys.setrecursionlimit(100000)
# === å®ä½“æå–æ¨¡å— ===
from extract_entity_file import extract_file_entity
from extract_entity_variable import extract_variable_entities, extract_function_parameters
from extract_entity_function import extract_function_entities
from extract_entity_struct import extract_struct_entities
from extract_entity_field import extract_field_entities
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import pickle
# === å…³ç³»æå–æ¨¡å— ===
from extract_relation_calls import extract_calls_relations
from extract_relation_assignedto import extract_assigned_to_relations
from extract_relation_contains import build_file_level_contains
from extract_relation_has_members import extract_has_member_relations
from extract_relation_has_parameters import extract_has_parameter_relations
from extract_relation_has_variables import extract_has_variable_relations
from extract_relation_returns import extract_returns_relations
from extract_relation_typeof import extract_typeof_relations
from extract_fail_message import extarct_mes
from extract_relation_alias import extract_alias_relations
# === åŒ…å«å…³ç³»æå–æ¨¡å— ===
from extract_relation_includes import extract_include_relations, build_transitive_includes, extract_extern_declarations

# === é…ç½®è·¯å¾„ ===
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
LANG_SO_PATH = os.path.join(ROOT_DIR, '..', 'build', 'my-languages.so')
OUTPUT_BASE = os.path.join(ROOT_DIR, '..', 'output')
MACRO_JSON_PATH = r"E:\cpppro\clang_kg\test\code_kg_with_tree-sitter\output\linux\macro_win.json"

def id_generator(start=1):
    while True:
        yield start
        start += 1

def get_parser():
    language = Language(tsc.language())
    parser = Parser(language)
    return parser

def get_c_files(directory):
    for root, _, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.c', '.h')):
                yield os.path.join(root, file)

def load_macro_lookup_map(json_path):
    if not os.path.exists(json_path):
        print(f"Warning: Macro file not found: {json_path}")
        return defaultdict(list)
        
    with open(json_path, 'r') as f:
        macro_json = json.load(f)
    macro_lookup_map = defaultdict(list)
    
    # ğŸ”§ ä¿®å¤ï¼šè·å– macro.json çš„ç›®å½•ä½œä¸ºåŸºç¡€è·¯å¾„
    macro_dir = os.path.dirname(json_path)
    
    for entry in macro_json:
        file = entry["file"]  # "./test_1.c" 
        start_line, start_col, end_line, end_col = entry["location"]
        macro_lookup_map[file].append({
            "range": ((start_line, start_col), (end_line, end_col)),
            "expanded": entry["macro"],
            "original": entry["name"],
            'extracted_lines': entry['extracted_lines']
        })
    return macro_lookup_map

def build_entity_file_mapping(all_entities):
    """æ„å»ºå®ä½“IDåˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„"""
    entity_file_map = {}
    
    for entity in all_entities:
        if entity.get('source_file'):
            abs_path = os.path.abspath(entity['source_file'])
            entity_file_map[entity['id']] = abs_path
        elif entity.get('type') == 'FILE':
            if entity.get('source_file'):
                abs_path = os.path.abspath(entity['source_file'])
            else:
                abs_path = os.path.abspath(entity['name'])
            entity_file_map[entity['id']] = abs_path
    
    return entity_file_map

def build_file_to_entities_mapping(all_entities):
    """ğŸš€ æ–°å¢ï¼šæ„å»ºæ–‡ä»¶åˆ°å®ä½“çš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾"""
    file_to_entities = defaultdict(list)
    
    for entity in all_entities:
        if entity.get('source_file'):
            abs_path = os.path.abspath(entity['source_file'])
            file_to_entities[abs_path].append(entity)
    
    return file_to_entities

# ========== ä¼˜åŒ–1ï¼šå‡å°‘æ•°æ®ä¼ é€’ï¼Œä½¿ç”¨å…¨å±€å˜é‡ï¼ˆé€‚ç”¨äºå¤šè¿›ç¨‹ï¼‰ ==========
_GLOBAL_SHARED_DATA = None

def init_worker(shared_data_path):
    """è¿›ç¨‹åˆå§‹åŒ–å‡½æ•°ï¼šä»ç£ç›˜åŠ è½½å…±äº«æ•°æ®"""
    global _GLOBAL_SHARED_DATA
    with open(shared_data_path, 'rb') as f:
        _GLOBAL_SHARED_DATA = pickle.load(f)


# ========== ä¼˜åŒ–2ï¼šè½»é‡çº§å·¥ä½œå‡½æ•°ï¼ˆåªä¼ æ–‡ä»¶è·¯å¾„ï¼‰ ==========
def process_calls_worker(source_path):
    """é˜¶æ®µ4ï¼šCALLSå…³ç³»æå–ï¼ˆè½»é‡çº§ç‰ˆæœ¬ï¼‰"""
    parser = get_parser()
    abs_source_path = os.path.abspath(source_path)
    
    try:
        with open(abs_source_path, 'rb') as f:
            code_bytes = f.read()
        tree = parser.parse(code_bytes)
        root = tree.root_node
        
        shared = _GLOBAL_SHARED_DATA
        rels = extract_calls_relations(
            root, code_bytes, 
            shared['function_id_map'], 
            shared['var_param_map'],
            shared['field_id_map'],
            source_path, 
            shared['file_visibility'], 
            shared['entity_file_map'], 
            shared['all_extern_functions'], 
            shared['macro_lookup_map'], 
            source_path, 
            shared['all_entities'],
            flag=True
        )
        gc.collect()
        del tree, root
        return rels
    except Exception as e:
        print(f"Error in {source_path}: {e}")
        return []

def process_assigned_worker(source_path):
    """é˜¶æ®µ5ï¼šASSIGNED_TOå…³ç³»æå–"""
    parser = get_parser()
    abs_source_path = os.path.abspath(source_path)
    
    try:
        with open(abs_source_path, 'rb') as f:
            code_bytes = f.read()
        tree = parser.parse(code_bytes)
        root = tree.root_node
        
        shared = _GLOBAL_SHARED_DATA
        rels = extract_assigned_to_relations(
            root, code_bytes,
            shared['function_id_map'],
            shared['var_param_map'],
            shared['field_id_map'],
            source_path,
            shared['file_visibility'],
            shared['entity_file_map'],
            shared['all_extern_functions'],
            shared['macro_lookup_map'],
            source_path,
            flag=True
        )
        
        del tree, root
        return rels
    except Exception as e:
        print(f"Error in {source_path}: {e}")
        return []

def init_worker(var_ent, par_ent, fld_ent, str_map, file_vis, ent_map):
    """
    åœ¨æ¯ä¸ªå­è¿›ç¨‹å¯åŠ¨æ—¶åˆå§‹åŒ–å…¨å±€å˜é‡
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨æ¯ä¸ªå·¥ä½œè¿›ç¨‹åˆ›å»ºæ—¶è¢«è°ƒç”¨ä¸€æ¬¡
    """
    global variable_entities, param_entities, field_entities
    global struct_id_map, file_visibility, entity_file_map
    
    variable_entities = var_ent
    param_entities = par_ent
    field_entities = fld_ent
    struct_id_map = str_map
    file_visibility = file_vis
    entity_file_map = ent_map


def parallel_extract_with_threads(c_files, shared_data, process_func, stage_name, num_workers=8):
    """ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼ˆé€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼‰"""
    print(f"\n{'='*60}")
    print(f"{stage_name} (ä½¿ç”¨ {num_workers} ä¸ªçº¿ç¨‹)")
    
    # è®¾ç½®å…¨å±€å…±äº«æ•°æ®ï¼ˆçº¿ç¨‹é—´å…±äº«å†…å­˜ï¼Œæ— åºåˆ—åŒ–å¼€é”€ï¼‰
    global _GLOBAL_SHARED_DATA
    _GLOBAL_SHARED_DATA = shared_data
    
    all_rels = []
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(process_func, f): f for f in c_files}
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(c_files), desc=stage_name) as pbar:
            for future in as_completed(futures):
                rels = future.result()
                all_rels.extend(rels)
                pbar.update(1)
    
    print(f"âœ… {stage_name} å®Œæˆï¼Œæå–åˆ° {len(all_rels)} æ¡å…³ç³»")
    return all_rels


def deduplicate_relations(relations):
    """å»é‡å…³ç³»åˆ—è¡¨"""
    seen = set()
    unique_relations = []
    
    for rel in relations:
        rel_key = (rel['head'], rel['tail'], rel['type'])
        if rel_key not in seen:
            seen.add(rel_key)
            unique_relations.append(rel)
    
    return unique_relations

def extract_all(source_dir, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    entity_path = os.path.join(output_dir, 'entity.json')
    relation_path = os.path.join(output_dir, 'relation.json')

    id_counter = id_generator()
    parser = get_parser()

    all_entities = []
    all_relations = []

    # === æ˜ å°„è¡¨ï¼šæ”¯æŒå¤šå€¼æ˜ å°„ ===
    function_id_map = {}      # name -> [id1, id2, ...] æ”¯æŒåŒåå‡½æ•°
    variable_id_map = {}      # (name, scope) -> id æˆ– [id1, id2, ...] æ”¯æŒåŒåå…¨å±€å˜é‡
    param_id_map = {}         # (name, scope) -> id 
    struct_id_map = {}        # (name, scope) -> [id1, id2, ...] æ”¯æŒåŒåç»“æ„ä½“
    field_id_map = {}         # name -> [id1, id2, ...] 
    variable_scope_map = {}

    function_entities = []
    param_entities = []
    variable_entities = []
    struct_entities = []
    field_entities = []

    file_trees = []
    file_id_map = {}

    # === æºç ä¸å®ä¿¡æ¯è¯»å– ===
    c_files = list(get_c_files(source_dir))
    with open(r'E:\cpppro\clang_kg\test\code_kg_with_tree-sitter\output\linux\dupfile.json', 'r', encoding='utf-8') as f:
        dup_file = json.load(f)

    cal_files = []
    for value in c_files:
        if value not in dup_file:
            cal_files.append(value)
    c_files = cal_files
    macro_lookup_map = load_macro_lookup_map(MACRO_JSON_PATH)
    print(f"âœ… è¯»å–å®å±•å¼€ä¿¡æ¯å®Œæˆï¼Œå…±åŒ…å«æ–‡ä»¶æ•°ï¼š{len(macro_lookup_map)}")

    # === é˜¶æ®µ 1ï¼šæå–æ‰€æœ‰å®ä½“ ===
    print(f"\n" + "="*60)
    print("é˜¶æ®µ 1ï¼šæå–æ‰€æœ‰å®ä½“")

    with open(r'E:\cpppro\clang_kg\test\code_kg_with_tree-sitter\output\linux\res\temp_en.json', 'r', encoding='utf-8') as f:
        all_entities = json.load(f)
    import pickle
    data_to_save = pickle.load(open(r'E:\cpppro\clang_kg\test\code_kg_with_tree-sitter\output\linux\name2id.pkl', 'rb'))
    function_id_map = data_to_save['function_id_map']
    variable_id_map = data_to_save['variable_id_map']
    param_id_map = data_to_save['param_id_map']
    struct_id_map = data_to_save['struct_id_map']
    field_id_map = data_to_save['field_id_map']
    variable_scope_map = data_to_save['variable_scope_map']
    file_id_map = data_to_save['file_id_map']
    entity_file_map = data_to_save['entity_file_map']
    file_visibility = data_to_save['file_visibility']
    all_extern_functions = data_to_save['all_extern_functions']
    all_include_relations = data_to_save['all_include_relations']
    function_entities = data_to_save['function_entities']
    param_entities = data_to_save['param_entities']
    variable_entities = data_to_save['variable_entities']
    struct_entities = data_to_save['struct_entities']
    field_entities = data_to_save['field_entities']


    var_param_map = {**variable_id_map, **param_id_map}
    
    shared_data = {
        'function_id_map': function_id_map,
        'var_param_map': var_param_map,
        'field_id_map': field_id_map,
        'struct_id_map': struct_id_map,
        'file_visibility': file_visibility,
        'entity_file_map': entity_file_map,
        'all_extern_functions': all_extern_functions,
        'macro_lookup_map': macro_lookup_map,
        'all_entities': all_entities,
        'var_param_entities': variable_entities + param_entities,
        'field_entities': field_entities
    }
    # === é˜¶æ®µ 4ï¼šå‡½æ•°è°ƒç”¨å…³ç³»ï¼ˆå¹¶è¡Œç‰ˆï¼‰ ===
    print(f"\n" + "="*60)
    print("é˜¶æ®µ 4ï¼šæå– CALLS å…³ç³»ï¼ˆå¹¶è¡Œï¼‰...")
    """
    calls_rels = parallel_extract_with_threads(
        c_files, shared_data, process_calls_worker,
        "é˜¶æ®µ 4ï¼šCALLS", 8
    )
    all_relations.extend(calls_rels)
    """
    for source_path in tqdm(c_files, desc="é˜¶æ®µ 4ï¼šæå– CALLS"):
        abs_source_path = os.path.abspath(source_path)

        if len(all_relations) % 1000 == 0:
            gc.collect()

        with open(abs_source_path, 'rb') as f:
            code_bytes = f.read()
        tree = parser.parse(code_bytes)
        root = tree.root_node

        rels = extract_calls_relations(
            root, code_bytes, function_id_map, {**variable_id_map, **param_id_map}, field_id_map,
            source_path, file_visibility, entity_file_map, all_extern_functions, macro_lookup_map, source_path, all_entities, flag=True
        )
        all_relations.extend(rels)

        del tree
        del root

    # === é˜¶æ®µ 5ï¼šèµ‹å€¼å…³ç³» ===
    print(f"\n" + "="*60)
    print("é˜¶æ®µ 5ï¼šæå– ASSIGNED_TO å…³ç³»...")
    
    for source_path in tqdm(c_files, desc="é˜¶æ®µ 5ï¼šæå– ASSIGNED_TO"):
        abs_source_path = os.path.abspath(source_path)
        
        if len(all_relations) % 1000 == 0:
            gc.collect()

        with open(abs_source_path, 'rb') as f:
            code_bytes = f.read()
        tree = parser.parse(code_bytes)
        root = tree.root_node

        rels = extract_assigned_to_relations(
            root, code_bytes, function_id_map, {**variable_id_map, **param_id_map}, field_id_map,
            source_path, file_visibility, entity_file_map, all_extern_functions, macro_lookup_map, source_path
        )
        all_relations.extend(rels)

        del tree
        del root
    
    # === é˜¶æ®µ 6ï¼šè¯­ä¹‰å…³ç³» ===
    print(f"\n" + "="*60)
    print("é˜¶æ®µ 6ï¼šæå– RETURNS / TYPE_OF...")
    
    for source_path in tqdm(c_files, desc="é˜¶æ®µ 6ï¼šæå– RETURNS / TYPE_OF"):
        abs_source_path = os.path.abspath(source_path)

        if len(all_relations) % 1000 == 0:
            gc.collect()

        with open(abs_source_path, 'rb') as f:
            code_bytes = f.read()
        tree = parser.parse(code_bytes)
        root = tree.root_node
        # RETURNS
        rels = extract_returns_relations(
            root, code_bytes, function_id_map, {**variable_id_map, **param_id_map}, field_id_map,
            source_path, file_visibility, entity_file_map
        )
        all_relations.extend(rels)

        # TYPE_OF
        rels = extract_typeof_relations(
            root, code_bytes, variable_entities + param_entities, field_entities, struct_id_map,
            source_path, file_visibility, entity_file_map
        )
        all_relations.extend(rels)

        del tree
        del root

    # æ¸…ç†å†…å­˜
    del file_trees
    del file_to_entities

    # === æœ€ç»ˆå»é‡å’Œç»Ÿè®¡ ===
    print(f"\n" + "="*60)
    print("å»é‡å…³ç³»...")
    original_count = len(all_relations)
    all_relations = deduplicate_relations(all_relations)
    deduplicated_count = len(all_relations)
    print(f"âœ… å»é‡å®Œæˆï¼š{original_count} -> {deduplicated_count} (ç§»é™¤ {original_count - deduplicated_count} ä¸ªé‡å¤)")

    # === è¾“å‡º JSON ===
    with open(entity_path, 'w') as f:
        json.dump(all_entities, f, indent=2)
    with open(relation_path, 'w') as f:
        json.dump(all_relations, f, indent=2)

    print(f"\nâœ… æå–å®Œæˆï¼šå®ä½“ {len(all_entities)} ä¸ªï¼Œå…³ç³» {len(all_relations)} æ¡ã€‚")
    
    # å…³ç³»ç»Ÿè®¡
    relation_types = Counter([r['type'] for r in all_relations])
    print(f"\nå…³ç³»ç±»å‹ç»Ÿè®¡ï¼š")
    for k, v in relation_types.items():
        print(f"  - {k}: {v}")
    
    # å¯è§æ€§æ£€æŸ¥ç»Ÿè®¡
    visibility_checked = sum(1 for r in all_relations if r.get('visibility_checked'))
    print(f"\nå¯è§æ€§æ£€æŸ¥è¦†ç›–ï¼š{visibility_checked}/{len(all_relations)} ({visibility_checked/len(all_relations)*100:.1f}%)")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--source", type=str, default=r'E:\cpppro\clang_kg\linux', help="C æºç ç›®å½•è·¯å¾„")
    parser.add_argument("--output", type=str, default=r'E:\cpppro\clang_kg\test\code_kg_with_tree-sitter\output\linux', help="è¾“å‡ºç›®å½•è·¯å¾„")
    args = parser.parse_args()

    tracemalloc.start()
    start_time = time.time()
    extract_all(args.source, args.output)
    current, peak = tracemalloc.get_traced_memory()
    end_time = time.time()
    print(f"\næ€»è€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")
    print(f"å½“å‰å†…å­˜ï¼š{current / 1024 / 1024:.2f} MBï¼›å³°å€¼ï¼š{peak / 1024 / 1024:.2f} MB")
    tracemalloc.stop()